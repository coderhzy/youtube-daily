"""
AI Processor for news content using OpenRouter
"""

from typing import List, Dict, Any
from openai import OpenAI

from src.config import (
    OPENROUTER_API_KEY,
    OPENROUTER_BASE_URL,
    OPENROUTER_MODEL,
    ENABLE_AI_SUMMARY,
    ARTICLE_TARGET_WORDS,
    ARTICLE_MAX_TOKENS
)
from src.utils.logger import get_logger

class AIProcessor:
    """AI processor using OpenRouter for content generation"""

    def __init__(self):
        self.logger = get_logger('ai_processor')

        if not OPENROUTER_API_KEY:
            raise ValueError("OPENROUTER_API_KEY must be set in environment variables")

        # åˆ›å»ºä¸å¸¦ä»£ç†çš„ httpx client
        import httpx
        http_client = httpx.Client(
            base_url=OPENROUTER_BASE_URL,
            timeout=120.0
        )

        self.client = OpenAI(
            api_key=OPENROUTER_API_KEY,
            base_url=OPENROUTER_BASE_URL,
            http_client=http_client
        )
        self.model = OPENROUTER_MODEL
        self.logger.info(f"AI Processor initialized with model: {self.model}")

    def process_daily_news(
        self,
        news_list: List[Dict[str, Any]],
        date_str: str
    ) -> Dict[str, Any]:
        """
        Process daily news and generate blog article

        Args:
            news_list: List of news items
            date_str: Date string (YYYY-MM-DD)

        Returns:
            Dict containing title, content, description, tags, attractive_title, cover_prompt
        """
        if not ENABLE_AI_SUMMARY:
            self.logger.info("AI summary is disabled, using basic formatting")
            return self._basic_format(news_list, date_str)

        try:
            self.logger.info(f"Processing {len(news_list)} news items with AI...")

            # Prepare news text
            news_text = self._prepare_news_text(news_list)

            # Call AI to generate article
            prompt = self._create_prompt(news_text, date_str)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½åŒºå—é“¾YouTubeåšä¸»ï¼Œæ“…é•¿ç”¨å£è¯­åŒ–ã€æ¥åœ°æ°”çš„æ–¹å¼è®²è§£è¡Œä¸šæ–°é—»ã€‚ä½ çš„é£æ ¼æ˜¯ï¼šç®€æ´ç›´æ¥ã€ä¸ç”¨ä¹¦é¢è¯­ã€åƒå’Œæœ‹å‹èŠå¤©ã€‚é¿å…ä½¿ç”¨'å€¼å¾—æ³¨æ„'ã€'æ€»è€Œè¨€ä¹‹'ã€'ä¸å¯å¦è®¤'ç­‰AIè…”è°ƒè¯æ±‡ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.8,
                max_tokens=ARTICLE_MAX_TOKENS
            )

            result = response.choices[0].message.content

            # Parse AI response
            parsed_result = self._parse_ai_response(result, date_str)

            # Generate attractive title and cover image prompt
            self.logger.info("Generating attractive title and cover image prompt...")
            title_and_cover = self._generate_attractive_title_and_cover(news_text, parsed_result['content'])

            parsed_result['attractive_title'] = title_and_cover['title']
            parsed_result['cover_prompt'] = title_and_cover['cover_prompt']

            self.logger.info("AI processing completed successfully")
            return parsed_result

        except Exception as e:
            self.logger.error(f"Error in AI processing: {e}")
            self.logger.info("Falling back to basic formatting")
            return self._basic_format(news_list, date_str)

    def _prepare_news_text(self, news_list: List[Dict[str, Any]]) -> str:
        """Prepare news text for AI processing"""
        news_items = []
        for i, news in enumerate(news_list, 1):
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            title = news.get('title', '')
            content = news.get('content', '')

            news_items.append(
                f"{i}. [{source}] {title}\n   {content}"
            )

        return "\n\n".join(news_items)

    def _create_prompt(self, news_text: str, date_str: str) -> str:
        """Create AI prompt"""
        return f"""è¯·å°†ä»¥ä¸‹åŒºå—é“¾æ–°é—»æ•´ç†æˆä¸€ç¯‡é€‚åˆTTSè¯­éŸ³åˆæˆçš„è§†é¢‘è„šæœ¬ã€‚

æ—¥æœŸ: {date_str}

æ–°é—»å†…å®¹:
{news_text}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚å¤„ç†:

1. **æ–‡ç« é•¿åº¦**: ç›®æ ‡{ARTICLE_TARGET_WORDS}å­—å·¦å³

2. **å†™ä½œæŠ€å·§ - ç°è±¡â†’é—®é¢˜â†’è§£ç­”**:
   åœ¨ä»Šå¤©æœ€é‡è¦çš„1-2ä¸ªè¯é¢˜ä¸Šï¼Œå¯ä»¥ç”¨è¿™ä¸ªç»“æ„æ¥å¢åŠ æ·±åº¦ï¼š
   - ç°è±¡: å®¢è§‚é™ˆè¿°å‘ç”Ÿäº†ä»€ä¹ˆ
   - é—®é¢˜: æå‡ºè§‚ä¼—å…³å¿ƒçš„é—®é¢˜ï¼ˆ"é‚£æˆ‘ä»¬ä¸ç¦è¦é—®..."ï¼‰
   - è§£ç­”: å¼•ç”¨ä¸“å®¶è§‚ç‚¹æˆ–åˆ†æ

   ç¤ºä¾‹:
   "è‹±ä¼Ÿè¾¾æ˜¨å¤œå¤§æ¶¨ç™¾åˆ†ä¹‹äº”ï¼Œçº³æŒ‡çªç ´ä¸¤ä¸‡ç‚¹ï¼Œå¼•çˆ†äº†æŠ•èµ„è€…å¯¹AIçš„çƒ­æƒ…ã€‚
   é‚£æˆ‘ä»¬ä¸ç¦è¦é—®ï¼ŒAIè¡Œæƒ…è¿˜èƒ½æŒç»­å¤šä¹…ã€‚
   è‘—åæŠ•èµ„äººæœ¨å¤´å§è®¤ä¸ºï¼ŒAIä»å¤„äºæ—©æœŸé˜¶æ®µã€‚"

   æ³¨æ„ï¼šä¸è¦æ¯ä¸ªè¯é¢˜éƒ½ç”¨è¿™ä¸ªç»“æ„ï¼Œå…¶ä»–æ¬¡è¦æ–°é—»ç®€å•é™ˆè¿°å³å¯ï¼Œä¿æŒè‡ªç„¶

3. **è¯­è¨€é£æ ¼**:
   - å¹³ç¨³å™è¿°ï¼Œåƒæ–°é—»æ’­æŠ¥
   - ç¦æ­¢æ„Ÿå¹å·å’Œå¤¸å¼ è¯
   - ç¦æ­¢"å€¼å¾—æ³¨æ„çš„æ˜¯"ã€"æ€»è€Œè¨€ä¹‹"ç­‰å¥—è¯
   - ç”¨çŸ­å¥ï¼Œç›´æ¥é™ˆè¿°

4. **æ•°å­—æ ¼å¼** (TTSæœ—è¯»):
   - æ‰€æœ‰æ•°å­—è½¬ä¸­æ–‡: "20%" â†’ "ç™¾åˆ†ä¹‹äºŒå"
   - é‡‘é¢: "$95000" â†’ "ä¹ä¸‡äº”åƒç¾å…ƒ"
   - æ—¥æœŸ: "11æœˆ28æ—¥" â†’ "åä¸€æœˆäºŒåå…«æ—¥"

5. **æ–‡ç« ç»“æ„**:
   a) **å¼€åœº**: "å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°åŒºå—é“¾æ¯æ—¥è§‚å¯Ÿ" + ä»Šå¤©æœ€é‡è¦çš„ä¸€ä¸¤ä»¶äº‹

   b) **äº’åŠ¨æç¤º**: "å¦‚æœè§‰å¾—æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç‚¹èµè®¢é˜…ï¼Œå¼€å¯å°é“ƒé“›"

   c) **ä¸»ä½“å†…å®¹** (2-3ä¸ªè¯é¢˜)

   d) **ç»“å°¾**: ç”¨è‡ªç„¶çš„è¿‡æ¸¡è¯­ï¼Œå¦‚"å¥½äº†ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹ä»Šå¤©çš„é‡ç‚¹"ï¼Œä¸è¦ç›´æ¥è¯´"æ€»ç»“"ä¸¤ä¸ªå­—

6. **æ ¼å¼è¦æ±‚**:
   - ä½¿ç”¨Markdownï¼Œæ¿å—ç”¨äºŒçº§æ ‡é¢˜
   - ä¸ç”¨emojiï¼Œä¸ç”¨ç²—ä½“æ˜Ÿå·

è¯·ç›´æ¥è¾“å‡ºè„šæœ¬å†…å®¹ã€‚"""

    def _parse_ai_response(self, ai_response: str, date_str: str) -> Dict[str, Any]:
        """Parse AI response"""
        lines = ai_response.strip().split('\n')
        description_lines = []
        content_start = 0

        # è¿‡æ»¤æ‰AIçš„ç¤¼è²Œæ€§å›å¤å¼€å¤´
        skip_phrases = [
            'å¥½çš„', 'è¿™æ˜¯', 'æ ¹æ®', 'æ‚¨æä¾›', 'æ•´ç†è€Œæˆ',
            'ä»¥ä¸‹æ˜¯', 'ä¸ºæ‚¨', 'æˆ‘å°†', 'è®©æˆ‘', '---', '**æ—¥æœŸï¼š'
        ]

        for i, line in enumerate(lines):
            if line.startswith('##'):
                content_start = i
                break

            # è·³è¿‡ç©ºè¡Œå’Œä»¥#å¼€å¤´çš„è¡Œ
            if not line.strip() or line.startswith('#'):
                continue

            # è·³è¿‡åŒ…å«ç¤¼è²Œæ€§å›å¤çš„è¡Œ
            if any(phrase in line for phrase in skip_phrases):
                continue

            # åªæ·»åŠ æœ‰å®é™…å†…å®¹çš„è¡Œ
            if line.strip():
                description_lines.append(line.strip())

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°
        if description_lines:
            description = ' '.join(description_lines[:2])  # åªå–å‰2è¡Œï¼Œé¿å…å¤ªé•¿
        else:
            description = f"åŒºå—é“¾æ¯æ—¥è§‚å¯Ÿ - {date_str}"

        content = '\n'.join(lines[content_start:]) if content_start > 0 else ai_response

        # å¼ºåˆ¶æ’å…¥äº’åŠ¨æç¤ºï¼ˆå¦‚æœAIæ²¡æœ‰ç”Ÿæˆï¼‰
        cta_text = "å¦‚æœä½ è§‰å¾—å†…å®¹æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç‚¹èµã€è®¢é˜…ï¼Œå¼€å¯å°é“ƒé“›ï¼Œä¸é”™è¿‡æ¯å¤©çš„è¡Œä¸šåŠ¨æ€"

        # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«äº’åŠ¨æç¤º
        if "ç‚¹èµ" not in content or "è®¢é˜…" not in content:
            self.logger.info("AIæœªç”Ÿæˆäº’åŠ¨æç¤ºï¼Œå¼ºåˆ¶æ’å…¥...")

            # åœ¨ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜å‰æ’å…¥äº’åŠ¨æç¤º
            lines_list = content.split('\n')
            insert_index = 0

            for i, line in enumerate(lines_list):
                if line.startswith('## '):
                    insert_index = i
                    break

            if insert_index > 0:
                # åœ¨ç¬¬ä¸€ä¸ªæ¿å—å‰æ’å…¥
                lines_list.insert(insert_index, f"\n{cta_text}\n")
                content = '\n'.join(lines_list)
                self.logger.info("äº’åŠ¨æç¤ºå·²æ’å…¥åˆ°ç¬¬ä¸€ä¸ªæ¿å—å‰")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¿å—æ ‡é¢˜ï¼Œæ’å…¥åˆ°å¼€å¤´å
                lines_list.insert(min(10, len(lines_list)), f"\n{cta_text}\n")
                content = '\n'.join(lines_list)
                self.logger.info("äº’åŠ¨æç¤ºå·²æ’å…¥åˆ°å¼€å¤´ä½ç½®")

        tags = self._extract_tags(content)

        return {
            'title': f"åŒºå—é“¾æ¯æ—¥è§‚å¯Ÿ - {date_str}",
            'content': content,
            'description': description[:200],  # é™åˆ¶200å­—ç¬¦
            'tags': tags
        }

    def _extract_tags(self, content: str) -> List[str]:
        """Extract tags from content"""
        tags = ['åŒºå—é“¾', 'æ¯æ—¥è§‚å¯Ÿ']

        content_lower = content.lower()

        if 'defi' in content_lower or 'å»ä¸­å¿ƒåŒ–é‡‘è' in content:
            tags.append('DeFi')
        if 'nft' in content_lower or 'æ•°å­—è—å“' in content:
            tags.append('NFT')
        if 'æ¯”ç‰¹å¸' in content or 'bitcoin' in content_lower or 'btc' in content_lower:
            tags.append('æ¯”ç‰¹å¸')
        if 'ä»¥å¤ªåŠ' in content or 'ethereum' in content_lower or 'eth' in content_lower:
            tags.append('ä»¥å¤ªåŠ')
        if 'ç›‘ç®¡' in content or 'æ”¿ç­–' in content:
            tags.append('æ”¿ç­–ç›‘ç®¡')
        if 'èèµ„' in content or 'æŠ•èµ„' in content:
            tags.append('æŠ•èèµ„')

        return list(set(tags))

    def _generate_attractive_title_and_cover(self, news_text: str, article_content: str) -> Dict[str, str]:
        """
        Generate attractive title and cover image prompt based on article content

        Args:
            news_text: Raw news text
            article_content: Processed article content

        Returns:
            Dict with 'title' and 'cover_prompt'
        """
        try:
            prompt = f"""åŸºäºä»¥ä¸‹åŒºå—é“¾æ–°é—»æ–‡ç« ï¼Œç”Ÿæˆä¸€ä¸ªå¸å¼•äººçš„YouTubeè§†é¢‘æ ‡é¢˜å’Œå°é¢å›¾æè¿°ã€‚

æ–‡ç« æ‘˜è¦:
{article_content[:1000]}...

è¦æ±‚:

1. **YouTubeè§†é¢‘æ ‡é¢˜**ï¼ˆ10-25ä¸ªå­—ï¼‰:
   - æŠ“ä½ä»Šæ—¥æœ€æ ¸å¿ƒã€æœ€å¸å¼•çœ¼çƒçš„è¯é¢˜
   - ä½¿ç”¨æ•°å­—ã€æƒ…æ„Ÿè¯ã€æ‚¬å¿µç­‰æŠ€å·§
   - é€‚åˆYouTubeç®—æ³•æ¨è
   - ç¤ºä¾‹: "æ¯”ç‰¹å¸æš´è·Œ20%ï¼å·¨é²¸å´åœ¨ç–¯ç‹‚æŠ„åº•ï¼Ÿ"
   - ç¤ºä¾‹: "ä»¥å¤ªåŠé‡å¤§å‡çº§ï¼Gasè´¹ç”¨é™ä½90%"
   - ç¤ºä¾‹: "ç¾å›½SECçªå‘æ–°æ”¿ï¼åŠ å¯†å¸‚åœºå°†è¿æ¥å·¨å˜ï¼Ÿ"

2. **å°é¢å›¾æç¤ºè¯**ï¼ˆè‹±æ–‡ï¼Œç”¨äºNano Banana Proå›¾ç‰‡ç”Ÿæˆï¼‰:
   - å¿…é¡»åŒ…å«å¸å¼•äººçš„äººç‰©åœºæ™¯
   - çªå‡ºä»Šæ—¥æœ€é‡è¦çš„ä¸»é¢˜
   - é€‚åˆYouTubeç¼©ç•¥å›¾ï¼ˆ16:9æ¨ªå±ï¼‰
   - åŒ…å«æ¸…æ™°çš„ä¸­æ–‡æ ‡é¢˜æ–‡å­—
   - è§†è§‰å†²å‡»åŠ›å¼ºï¼Œè®©äººæƒ³ç‚¹å‡»
   - ç¬¦åˆNano Banana Proçš„æ–‡å­—æ¸²æŸ“ä¼˜åŠ¿

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
  "title": "å¸å¼•äººçš„YouTubeæ ‡é¢˜",
  "cover_prompt": "Detailed English prompt for cover image generation"
}}

å°é¢å›¾æç¤ºè¯ç¤ºä¾‹æ ¼å¼:
"Create a dramatic YouTube thumbnail image for blockchain news.
Scene: Shocked/excited business analyst looking at large digital screen showing [KEY EVENT],
dramatic lighting, urgent atmosphere, close-up professional photography style.
Chinese title text: '[YouTubeæ ‡é¢˜]' in large bold white characters (90pt+) at top,
highly visible and readable using Nano Banana Pro's superior text rendering.
Visual elements: Bitcoin/crypto symbols, price charts with dramatic arrows,
breaking news aesthetic, red/green color accents for market emotions.
Background: Gradient dark blue to black, cinematic lighting, high contrast.
Style: YouTube thumbnail optimized, attention-grabbing, professional yet dramatic,
perfect for video presentation, 16:9 horizontal format.
Text rendering: Ensure Chinese characters are crystal clear and prominent using
Nano Banana Pro's industry-leading multilingual text capabilities."

æ³¨æ„: æ ‡é¢˜è¦èƒ½å¼•èµ·å¥½å¥‡å¿ƒå’Œç‚¹å‡»æ¬²æœ›ï¼Œå°é¢å›¾è¦è§†è§‰å†²å‡»åŠ›å¼ºï¼"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.8,  # ç¨é«˜çš„æ¸©åº¦ä»¥è·å¾—æ›´æœ‰åˆ›æ„çš„æ ‡é¢˜
                max_tokens=2000
            )

            result = response.choices[0].message.content

            # Parse JSON response
            import json
            import re

            # Extract JSON from markdown code blocks if present
            json_match = re.search(r'```json\s*(.*?)\s*```', result, re.DOTALL)
            if json_match:
                result = json_match.group(1)

            parsed = json.loads(result)

            self.logger.info(f"Generated attractive title: {parsed['title']}")
            return parsed

        except Exception as e:
            self.logger.error(f"Error generating attractive title and cover: {e}")
            # Fallback
            return {
                'title': 'ä»Šæ—¥åŒºå—é“¾è¡Œä¸šé‡å¤§åŠ¨æ€',
                'cover_prompt': 'Professional blockchain news presenter looking at dramatic market charts, urgent atmosphere, Chinese title text visible'
            }

    def _basic_format(self, news_list: List[Dict[str, Any]], date_str: str) -> Dict[str, Any]:
        """Basic formatting without AI"""
        content_parts = [
            f"# åŒºå—é“¾æ¯æ—¥è§‚å¯Ÿ - {date_str}\n",
            f"ä»Šæ—¥å…±æ”¶å½• {len(news_list)} æ¡åŒºå—é“¾è¡Œä¸šåŠ¨æ€\n",
            "---\n"
        ]

        # Group by source
        sources = {}
        for news in news_list:
            source = news.get('source', 'å…¶ä»–')
            if source not in sources:
                sources[source] = []
            sources[source].append(news)

        for source, source_news in sources.items():
            content_parts.append(f"\n## ğŸ“° {source}\n")
            for news in source_news:
                content_parts.append(f"- **{news['title']}**  \n  {news['content']}\n")

        content = '\n'.join(content_parts)

        return {
            'title': f"åŒºå—é“¾æ¯æ—¥è§‚å¯Ÿ - {date_str}",
            'content': content,
            'description': f"åŒºå—é“¾æ¯æ—¥è§‚å¯Ÿ - {date_str},æ”¶å½•{len(news_list)}æ¡è¡Œä¸šåŠ¨æ€",
            'tags': ['åŒºå—é“¾', 'æ¯æ—¥è§‚å¯Ÿ'],
            'attractive_title': f'ä»Šæ—¥åŒºå—é“¾è¡Œä¸šåŠ¨æ€ - {date_str}',
            'cover_prompt': 'Professional blockchain news, modern office setting'
        }
